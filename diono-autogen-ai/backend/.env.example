# Application
APP_NAME=DionoAutogen AI
DEBUG=false

# API
API_HOST=0.0.0.0
API_PORT=8000

# CORS
CORS_ORIGINS=["http://localhost:3000","http://localhost:4000"]

# LLM Configuration
# Supported providers: ollama, openai, gemini, huggingface, qwen, deepseek, anthropic, groq, together, openrouter
LLM_PROVIDER=ollama
LLM_MODEL=mistral:instruct

# LLM_API_URL should be the base URL without /v1 path
# The system will append /v1/chat/completions automatically for OpenAI-compatible providers
# WARNING: Do not include /v1 in the URL - it will be added automatically
# Correct: http://localhost:11434
# Wrong: http://localhost:11434/v1
LLM_API_URL=http://localhost:11434
LLM_API_KEY=

# LLM Parameters
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=4096

# Provider-Specific Examples:
# OpenAI: LLM_PROVIDER=openai, LLM_API_URL=https://api.openai.com, LLM_MODEL=gpt-4
# Gemini: LLM_PROVIDER=gemini, LLM_API_URL=https://generativelanguage.googleapis.com, LLM_MODEL=gemini-pro
# Qwen: LLM_PROVIDER=qwen, LLM_API_URL=https://dashscope.aliyuncs.com, LLM_MODEL=qwen-turbo
# DeepSeek: LLM_PROVIDER=deepseek, LLM_API_URL=https://api.deepseek.com, LLM_MODEL=deepseek-chat
# Groq: LLM_PROVIDER=groq, LLM_API_URL=https://api.groq.com, LLM_MODEL=mixtral-8x7b-32768

# Workspace
WORKSPACE_DIR=/workspace
# HOST_WORKSPACE_PATH is set automatically by docker-compose for Docker-in-Docker
# If running outside Docker, set this to the absolute path of your workspace directory
# HOST_WORKSPACE_PATH=/path/to/workspace
MAX_WORKSPACE_SIZE_MB=1000

# Execution
MAX_EXECUTION_TIME=300
SANDBOX_MEMORY_LIMIT=2g
SANDBOX_CPU_LIMIT=2.0
SANDBOX_MAX_WORKERS=4
SANDBOX_NETWORK_ENABLED=true

# Security
SECRET_KEY=change-this-to-a-random-secret-key
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30

# Cloud Storage (optional)
GOOGLE_DRIVE_CREDENTIALS=
DROPBOX_ACCESS_TOKEN=

# Logging
LOG_LEVEL=INFO
LOG_FILE=logs/diono_autogen.log
